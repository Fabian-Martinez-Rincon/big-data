{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75fc5fe6-6e5d-48b6-bad8-cecc1a2d459e",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "\n",
    "# Practica 2\n",
    "\n",
    "<img src='https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExbXVoOXh6bGtwend4cW5majMwZHRiYzh6b2loZGNseGZmajMxbmhsNiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/aMwdfGSyeYjUKY6vjf/giphy.gif'>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318478e-c2e9-49ab-a66d-ad6d3a8f6486",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio 1)\n",
    "¿En el dataset del ejercicio 1 de la práctica 1 indique para cada Job, si se vería beneficiado por una función combiner?\n",
    "\n",
    "En caso afirmativo,\n",
    "\n",
    "¿cuál es la implementación de dicha función? ¿Qué datos recibe cada reduce, al utilizar la función combiner?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ca962-7047-420f-aa38-ada70ee73fb0",
   "metadata": {},
   "source": [
    "## Ejercicio 2)\n",
    "Implemente una función combiner para el problema del WordCount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "862c096a-e224-4355-a583-e3a60a91c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[MRE] INICIANDO ETAPA DE MAPEO...\u001b[0m\n",
      "\u001b[94m[Map]\u001b[0m EL -> 1\n",
      "\u001b[94m[Map]\u001b[0m PAPAGAYO -> 1\n",
      "\u001b[94m[Map]\u001b[0m DE -> 1\n",
      "\u001b[94m[Map]\u001b[0m HUICHILOBOS -> 1\n",
      "\u001b[94m[Map]\u001b[0m Repentinamente -> 1\n",
      "\u001b[94m[Map]\u001b[0m oí -> 1\n",
      "\u001b[94m[Map]\u001b[0m una -> 1\n",
      "\u001b[94m[Map]\u001b[0m exclamación -> 1\n",
      "\u001b[94m[Map]\u001b[0m de -> 1\n",
      "\u001b[94m[Map]\u001b[0m sorpresa -> 1\n",
      "\u001b[94m... [más resultados de map omitidos]\u001b[0m\n",
      "\u001b[96m[MRE] INICIANDO ETAPA DE REDUCCIÓN...\u001b[0m\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: !ah! -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: \"A\" -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: \"B\" -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: \"Dr. -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: \"El -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: \"purgar\", -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: (aquÃ­ -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: (con -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: (naturalmente) -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: * -> [11, 1, 1, 1, 1]\n",
      "\u001b[96m... [más reduce claves omitidas]\u001b[0m\n",
      "\u001b[93m[MRE] FINALIZANDO Y ESCRIBIENDO RESULTADOS EN DISCO...\u001b[0m\n",
      "\u001b[92m\n",
      "[MRE] RESULTADOS FINALES DEL JOB:\u001b[0m\n",
      "\u001b[95m!ah!\u001b[0m\t1\n",
      "\u001b[95m\"A\"\u001b[0m\t1\n",
      "\u001b[95m\"B\"\u001b[0m\t1\n",
      "\u001b[95m\"Dr.\u001b[0m\t1\n",
      "\u001b[95m\"El\u001b[0m\t1\n",
      "\u001b[95m\"purgar\",\u001b[0m\t1\n",
      "\u001b[95m(aquÃ­\u001b[0m\t1\n",
      "\u001b[95m(con\u001b[0m\t1\n",
      "\u001b[95m(naturalmente)\u001b[0m\t1\n",
      "\u001b[95m*\u001b[0m\t5\n",
      "\u001b[91m... [más resultados omitidos]\u001b[0m\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "from MRE import Job\n",
    "\n",
    "inputDir = \"./WordCount/input/\"\n",
    "outputDir = \"./WordCount/ejercicio_2/\"\n",
    "\n",
    "def fmap(key, value, context):\n",
    "    words = value.split()\n",
    "    for w in words:\n",
    "        context.write(w, 1)\n",
    "        \n",
    "def fred(key, values, context):\n",
    "    c=0\n",
    "    for v in values:\n",
    "        c=c+1\n",
    "    context.write(key, c)\n",
    "\n",
    "job = Job(inputDir, outputDir, fmap, fred)\n",
    "job.setCombiner(fred)\n",
    "success = job.waitForCompletion()\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209636de-177f-4293-91f8-a2f53e096e3e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio 3)\n",
    "Implemente un job MapReduce para calcular el máximo, mínimo, promedio y desvío stándard de las ocurrencias de todas las palabras del dataset Libros.\n",
    "\n",
    "#### Cuenta ocurrencias de cada palabra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac60f1c-c5c4-4a5b-972a-c191e92c6b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[MRE] INICIANDO ETAPA DE MAPEO...\u001b[0m\n",
      "\u001b[94m[Map]\u001b[0m tristis -> 1\n",
      "\u001b[94m[Map]\u001b[0m imago -> 1\n",
      "\u001b[94m[Map]\u001b[0m --¡rafael -> 1\n",
      "\u001b[94m[Map]\u001b[0m exclamé -> 1\n",
      "\u001b[94m[Map]\u001b[0m pero -> 1\n",
      "\u001b[94m[Map]\u001b[0m él -> 1\n",
      "\u001b[94m[Map]\u001b[0m me -> 1\n",
      "\u001b[94m[Map]\u001b[0m interrumpió -> 1\n",
      "\u001b[94m[Map]\u001b[0m diciendo -> 1\n",
      "\u001b[94m[Map]\u001b[0m el -> 1\n",
      "\u001b[94m... [más resultados de map omitidos]\u001b[0m\n",
      "\u001b[96m[MRE] INICIANDO ETAPA DE REDUCCIÓN...\u001b[0m\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: * -> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, '...']\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: --\"estimado -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: --amigo -> [1, 1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: --angustias -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: --aquí -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: --aun -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: --buenas -> [1, 1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: --buenos -> [1, 1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: --comprendo -> [1]\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: --doctor -> [1]\n",
      "\u001b[96m... [más reduce claves omitidas]\u001b[0m\n",
      "\u001b[93m[MRE] FINALIZANDO Y ESCRIBIENDO RESULTADOS EN DISCO...\u001b[0m\n",
      "\u001b[92m\n",
      "[MRE] RESULTADOS FINALES DEL JOB:\u001b[0m\n",
      "\u001b[95m*\u001b[0m\t15\n",
      "\u001b[95m--\"estimado\u001b[0m\t1\n",
      "\u001b[95m--amigo\u001b[0m\t2\n",
      "\u001b[95m--angustias\u001b[0m\t1\n",
      "\u001b[95m--aquí\u001b[0m\t1\n",
      "\u001b[95m--aun\u001b[0m\t1\n",
      "\u001b[95m--buenas\u001b[0m\t2\n",
      "\u001b[95m--buenos\u001b[0m\t2\n",
      "\u001b[95m--comprendo\u001b[0m\t1\n",
      "\u001b[95m--doctor\u001b[0m\t1\n",
      "\u001b[91m... [más resultados omitidos]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "from MRE import Job\n",
    "\n",
    "inputDir = \"./Libros/input\"\n",
    "wcDir = \"./Libros/ej3_wc/\"\n",
    "\n",
    "def fmap_wc(key, value, context):\n",
    "    # separar por espacios\n",
    "    for palabra in value.strip().split():\n",
    "        palabra = palabra.lower().strip(\".,;:¡!¿?\\\"()[]\")\n",
    "        if palabra:\n",
    "            context.write(palabra, 1)\n",
    "\n",
    "def fred_wc(key, values, context):\n",
    "    total = 0\n",
    "    for v in values:\n",
    "        total += v\n",
    "    context.write(key, total)\n",
    "\n",
    "job_wc = Job(inputDir, wcDir, fmap_wc, fred_wc)\n",
    "job_wc.waitForCompletion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbd28d-0c99-4203-8e2f-e66d8c5cd8a3",
   "metadata": {},
   "source": [
    "#### Job de estadísticas (min, max, promedio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e90ef1ab-d50d-4cee-bd99-d36ba947177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[MRE] INICIANDO ETAPA DE MAPEO...\u001b[0m\n",
      "\u001b[94m[Map]\u001b[0m min -> 15\n",
      "\u001b[94m[Map]\u001b[0m max -> 15\n",
      "\u001b[94m[Map]\u001b[0m avg -> 15\n",
      "\u001b[94m[Map]\u001b[0m min -> 1\n",
      "\u001b[94m[Map]\u001b[0m max -> 1\n",
      "\u001b[94m[Map]\u001b[0m avg -> 1\n",
      "\u001b[94m[Map]\u001b[0m min -> 1\n",
      "\u001b[94m[Map]\u001b[0m max -> 1\n",
      "\u001b[94m[Map]\u001b[0m avg -> 1\n",
      "\u001b[94m[Map]\u001b[0m min -> 1\n",
      "\u001b[94m... [más resultados de map omitidos]\u001b[0m\n",
      "\u001b[96m[MRE] INICIANDO ETAPA DE REDUCCIÓN...\u001b[0m\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: avg -> [15, 1, 1, 1, 1, 1, 1, 1, 1, 3, '...']\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: max -> [15, 1, 1, 1, 1, 1, 1, 1, 1, 3, '...']\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: min -> [15, 1, 1, 1, 1, 1, 1, 1, 1, 3, '...']\n",
      "\u001b[93m[MRE] FINALIZANDO Y ESCRIBIENDO RESULTADOS EN DISCO...\u001b[0m\n",
      "\u001b[92m\n",
      "[MRE] RESULTADOS FINALES DEL JOB:\u001b[0m\n",
      "\u001b[95mpromedio\u001b[0m\t3.4946911196911197\n",
      "\u001b[95mmax\u001b[0m\t839\n",
      "\u001b[95mmin\u001b[0m\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statsDir = \"./Libros/ej3_stats/\"\n",
    "\n",
    "def fmap_stats(key, value, context):\n",
    "    if value.strip():  # saltear líneas vacías\n",
    "        ocurr = int(value.strip())\n",
    "        context.write(\"min\", ocurr)\n",
    "        context.write(\"max\", ocurr)\n",
    "        context.write(\"avg\", ocurr)\n",
    "\n",
    "\n",
    "def fred_stats(key, values, context):\n",
    "    if key == \"min\":\n",
    "        context.write(\"min\", min(values))\n",
    "    elif key == \"max\":\n",
    "        context.write(\"max\", max(values))\n",
    "    elif key == \"avg\":\n",
    "        total, n = 0, 0\n",
    "        for v in values:\n",
    "            total += v\n",
    "            n += 1\n",
    "        context.write(\"promedio\", total / n)\n",
    "\n",
    "job_stats = Job(wcDir, statsDir, fmap_stats, fred_stats)\n",
    "job_stats.waitForCompletion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efaeab3-7370-4c65-b20f-cc095789f4d2",
   "metadata": {},
   "source": [
    "#### Job de desvío estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1b44229-6789-4562-9843-22183b597ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[MRE] INICIANDO ETAPA DE MAPEO...\u001b[0m\n",
      "\u001b[94m[Map]\u001b[0m var -> 132.3721324313144\n",
      "\u001b[94m[Map]\u001b[0m var -> 6.223483782665732\n",
      "\u001b[94m[Map]\u001b[0m var -> 6.223483782665732\n",
      "\u001b[94m[Map]\u001b[0m var -> 6.223483782665732\n",
      "\u001b[94m[Map]\u001b[0m var -> 6.223483782665732\n",
      "\u001b[94m[Map]\u001b[0m var -> 6.223483782665732\n",
      "\u001b[94m[Map]\u001b[0m var -> 6.223483782665732\n",
      "\u001b[94m[Map]\u001b[0m var -> 6.223483782665732\n",
      "\u001b[94m[Map]\u001b[0m var -> 6.223483782665732\n",
      "\u001b[94m[Map]\u001b[0m var -> 0.24471930390125368\n",
      "\u001b[94m... [más resultados de map omitidos]\u001b[0m\n",
      "\u001b[96m[MRE] INICIANDO ETAPA DE REDUCCIÓN...\u001b[0m\n",
      "\u001b[96m[Reduce]\u001b[0m Clave recibida: var -> [132.3721324313144, 6.223483782665732, 6.223483782665732, 6.223483782665732, 6.223483782665732, 6.223483782665732, 6.223483782665732, 6.223483782665732, 6.223483782665732, 0.24471930390125368, '...']\n",
      "\u001b[93m[MRE] FINALIZANDO Y ESCRIBIENDO RESULTADOS EN DISCO...\u001b[0m\n",
      "\u001b[92m\n",
      "[MRE] RESULTADOS FINALES DEL JOB:\u001b[0m\n",
      "\u001b[95mdesvio\u001b[0m\t22.917022729315015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desvioDir = \"./Libros/ej3_desvio/\"\n",
    "\n",
    "import math\n",
    "\n",
    "def fmap_desvio(key, value, context):\n",
    "    if value.strip():  # saltear líneas vacías\n",
    "        ocurr = int(value.strip())\n",
    "        avg = context[\"avg\"]\n",
    "        context.write(\"var\", (ocurr - avg) ** 2)\n",
    "\n",
    "def fred_desvio(key, values, context):\n",
    "    total, n = 0, 0\n",
    "    for v in values:\n",
    "        total += v\n",
    "        n += 1\n",
    "    # varianza muestral\n",
    "    var = total / (n - 1)\n",
    "    context.write(\"desvio\", math.sqrt(var))\n",
    "\n",
    "# leer promedio del job_stats\n",
    "with open(statsDir + \"output.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "prom_line = [l for l in lines if l.startswith(\"promedio\")][0]\n",
    "promedio = float(prom_line.strip().split('\\t')[1])\n",
    "\n",
    "params = {\"avg\": promedio}\n",
    "job_desvio = Job(wcDir, desvioDir, fmap_desvio, fred_desvio)\n",
    "job_desvio.setParams(params)\n",
    "job_desvio.waitForCompletion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcfe01d-a5b6-4ef3-920c-a150424e3608",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio 4\n",
    "Utilice el dataset Libros para implementar una aplicación MapReduce que devuelva como salida todos los párrafos que tienen una longitud mayor al promedio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec15c4b-9c10-4632-a7a2-fd92672b0ec2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio 5\n",
    "\n",
    "El dataset website tiene información sobre el tiempo de permanencia de sus usuarios\n",
    "en cada una de las páginas del sitio. El formato de los datos del dataset es:\n",
    "<id_user, id_page, time>\n",
    "Implemente una aplicación MapReduce, utilizando combiners en los casos que\n",
    "considere necesario, que calcule\n",
    "\n",
    "> Indique como queda el DAG del proceso completo (las tres consultas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9723344e-ca9b-45bb-9ace-f13c21f96808",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### a. La página más visitada (la página en la que más tiempo permaneció) para cada usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c7a132-a434-4ebe-a719-6483a1b82680",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### b. El usuario que más páginas distintas visitó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c7dc4-a7dd-414f-b647-4eb6000d7b54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### c. La página más visitada (en cuanto a cantidad de visitas, sin importar el tiempo de permanencia) por todos los usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0d5ca-3366-409a-a318-f5c209773fca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6) Cómo plantearía una solución MapReduce a los siguientes algoritmos secuenciales:\n",
    "\n",
    "i. entrada\n",
    "\n",
    "```\n",
    "textos: array [1..N] of string (dataset libros)\n",
    "```\n",
    "ii. algoritmo\n",
    "\n",
    "```python\n",
    "a={}; b={}; N = len(textos)\n",
    "for l in textos:\n",
    "    words = l.split()\n",
    "    for w in words:\n",
    "        a[w] = a[w]+1\n",
    "\n",
    "for w in a.keys():\n",
    "    for l in lines:\n",
    "        words = l.split()\n",
    "        if w in words:\n",
    "            b[w]=b[w]+1\n",
    "for k in a.keys():\n",
    "    print(k + \" = \" + str(a[w] * (N / b[w])))\n",
    "```\n",
    "\n",
    "b.\n",
    "i. entrada\n",
    "```\n",
    "datos: array [1..N] of /<int1, int2, ..., intM> (todos los valores están dentro de un rango de valores conocido, para poder usarlos como índices del tensor)\n",
    "```\n",
    "ii. algoritmo\n",
    "\n",
    "```python\n",
    "for t in datos:\n",
    "    v = t.split(\"\\t\")\n",
    "    c = v[-1]\n",
    "        for a in range(len(v)-1):\n",
    "        x= v[a]\n",
    "        m[a][x][c] = m[a][x][c] + 1\n",
    "\n",
    "max=[[0,0,0], [0,0,0]]\n",
    "for x in range(len(m)):\n",
    "    for y in range(len(m[0])):\n",
    "        for z in range(2):\n",
    "            if(m[x][y][z] > max[z][0]):\n",
    "                max[z][0] = m[x][y][z]\n",
    "                max[z][1]=x\n",
    "                max[z][2]=y\n",
    "\n",
    "for z in range(2):\n",
    "    print(z +\";\" + max[z][1] +\";\" + max[z][2])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f6aa4-cfd5-4146-b041-28fdc562dfac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
